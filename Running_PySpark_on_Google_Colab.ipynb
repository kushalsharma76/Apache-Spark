{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Running PySpark on Google Colab",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGpvtpQM8kIk"
      },
      "source": [
        "# Running PySpark on Google Colab\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.4.7 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIV0fz1D5nP3"
      },
      "source": [
        "#Java Installation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-bUD7x_6Jv4"
      },
      "source": [
        "#Download Apache Spark Package\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UIFFhjL2wM8"
      },
      "source": [
        "#Extract the Spark Package\n",
        "!tar -xf spark-2.4.7-bin-hadoop2.7.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVhFnzwf3Eds",
        "outputId": "ddbd82b4-6f4c-453b-aa26-d1161730bcee"
      },
      "source": [
        "# Install PySpark and FindSpark packages to access the functionality of Spark\n",
        "!pip install -q pyspark\n",
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.3MB 72kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 34.9MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68F6jFp6bcM"
      },
      "source": [
        "#Set the environment variables for JAVA and SPARK\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtlQJQZ96Ebt"
      },
      "source": [
        "# Import FindSpark and PySpark\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "# Create sc - SparkContext object to start creating RDDs and performing transformation and actions\n",
        "sc = pyspark.SparkContext(appName='TestApp')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A3Xk8Rc9g9B"
      },
      "source": [
        "# Create a list\n",
        "l = [1,2,3,4]\n",
        "# Create an RDD from list l using parallelize function\n",
        "data = sc.parallelize(l)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpCVrTM69qNW",
        "outputId": "9ff99596-479a-4ea5-f75e-3ead8de9a3b6"
      },
      "source": [
        "# Perform action - count() on the RDD - data\n",
        "data.count()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvyIzucy9smp",
        "outputId": "5ea93152-6c33-4a6e-942c-c9ae9368d8dd"
      },
      "source": [
        "# Perform action - collect() on the RDD - data\n",
        "data.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfF6FHBZ9zLI"
      },
      "source": [
        "That's It!!"
      ]
    }
  ]
}